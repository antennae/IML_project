{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, r2_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Extraction: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300, 400, 3)       0         \n",
      "_________________________________________________________________\n",
      "inception_resnet_v2 (Model)  (None, 8, 11, 1536)       54336736  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1536)              0         \n",
      "=================================================================\n",
      "Total params: 54,336,736\n",
      "Trainable params: 54,276,192\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "number_of_elements = 10000  \n",
    "    \n",
    "inputs = keras.Input(shape=(300, 400, 3))\n",
    "base_model = keras.applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(300, 400, 3))\n",
    "x = base_model(inputs)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "model = keras.Model(inputs,x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_db = np.zeros([number_of_elements,1536])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction: [#######-------------] 32.7%\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_elements):\n",
    "  img_path = 'food/'+str(i).zfill(5)+'.jpg'\n",
    "  img = image.load_img(img_path, target_size=(300, 400))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  features = model.predict(x).astype(np.float32)\n",
    "\n",
    "  features_db[i,:] = features.reshape(1, 1536)\n",
    "\n",
    "  update_progress(i / number_of_elements)\n",
    "update_progress(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "print(features_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('features_db_InceptionResNetV2.csv', features_db, delimiter=',', comments = '')\n",
    "# The features database has been saved so that each time we can simply load the datatbase instead \n",
    "# of extracting it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_db = np.loadtxt('features_db_InceptionResNetV2.csv', delimiter = ',', dtype = np.float32)\n",
    "print(features_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_index = np.loadtxt('train_triplets.txt', delimiter = ' ').astype(int)\n",
    "print(Train_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_num = 500\n",
    "\n",
    "Training_set = Train_index[0:original_train_num,:]\n",
    "image_exist = np.unique(Training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_exist.shape)\n",
    "print(image_exist)\n",
    "print(Training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_set = np.array([]).reshape(0,3)\n",
    "image_val_exist = np.unique(Validation_set)\n",
    "print(image_val_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(original_train_num, Train_index.shape[0]):\n",
    "    update_progress((i- original_train_num)/( Train_index.shape[0] - original_train_num))\n",
    "    \n",
    "    mask = np.isin(Train_index[i,:], image_exist)\n",
    "    mask_val = np.isin(Train_index[i,:], image_val_exist)\n",
    "              \n",
    "    if mask.any() and (not mask_val.any()):\n",
    "        Training_set = np.append(Training_set, Train_index[i,:].reshape(1,3),axis = 0)\n",
    "        image_exist = np.unique(Training_set)\n",
    "      \n",
    "    if not mask.any():\n",
    "        Validation_set = np.append(Validation_set, Train_index[i,:].reshape(1,3), axis = 0)\n",
    "        image_val_exist = np.unique(Validation_set)\n",
    "\n",
    "update_progress(1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "print(Validation_set.shape)\n",
    "image_val_exist = np.unique(Validation_set)\n",
    "print(image_val_exist.shape)\n",
    "\n",
    "print('Training')\n",
    "print(Training_set.shape)\n",
    "mask = np.isin(image_val_exist, image_exist)\n",
    "\n",
    "print(np.count_nonzero(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(Training_set)\n",
    "np.random.shuffle(Validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_index_X = np.zeros([Training_set.shape[0], 3])\n",
    "\n",
    "for i in range(Training_set.shape[0]):\n",
    "  if i < Training_set.shape[0]//2:\n",
    "    data_train_index_X[i,:] = Training_set[i,:]\n",
    "  else:\n",
    "    data_train_index_X[i,0] = Training_set[i,0]\n",
    "    data_train_index_X[i,1] = Training_set[i,2]\n",
    "    data_train_index_X[i,2] = Training_set[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_index_X = np.zeros([Validation_set.shape[0], 3])\n",
    "\n",
    "for i in range(Validation_set.shape[0]):\n",
    "  if i < Validation_set.shape[0]//2:\n",
    "    data_val_index_X[i,:] = Validation_set[i,:]\n",
    "  else:\n",
    "    data_val_index_X[i,0] = Validation_set[i,0]\n",
    "    data_val_index_X[i,1] = Validation_set[i,2]\n",
    "    data_val_index_X[i,2] = Validation_set[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_label_Y = np.zeros(Training_set.shape[0])\n",
    "for i in range(Training_set.shape[0]//2):\n",
    "    data_train_label_Y[i] = 1\n",
    "    \n",
    "print(data_train_label_Y.shape)\n",
    "print(data_train_label_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_label_Y = np.zeros(Validation_set.shape[0])\n",
    "for i in range(Validation_set.shape[0]//2):\n",
    "    data_val_label_Y[i] = 1\n",
    "    \n",
    "print(data_val_label_Y.shape)\n",
    "print(data_val_label_Y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_train = np.zeros([data_train_index_X.shape[0],3,features_db.shape[1]])\n",
    "\n",
    "for i in range(data_train_index_X.shape[0]):\n",
    "    features_data_train[i,0,:] = features_db[int(data_train_index_X[i,0]),:]\n",
    "    features_data_train[i,1,:] = features_db[int(data_train_index_X[i,1]),:]\n",
    "    features_data_train[i,2,:] = features_db[int(data_train_index_X[i,2]),:]\n",
    "    \n",
    "print(features_data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_val = np.zeros([data_val_index_X.shape[0],3,features_db.shape[1]])\n",
    "\n",
    "for i in range(data_val_index_X.shape[0]):\n",
    "    features_data_val[i,0,:] = features_db[int(data_val_index_X[i,0]),:]\n",
    "    features_data_val[i,1,:] = features_db[int(data_val_index_X[i,1]),:]\n",
    "    features_data_val[i,2,:] = features_db[int(data_val_index_X[i,2]),:]\n",
    "    \n",
    "print(features_data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_train = features_data_train.reshape(features_data_train.shape[0],-1)\n",
    "print(\"training set reshaped\")\n",
    "features_data_val = features_data_val.reshape(features_data_val.shape[0],-1)\n",
    "print(\"validation set reshaped\")\n",
    "                \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features_data_train)\n",
    "print(\"scaler fitted\")\n",
    "features_data_train = scaler.transform(features_data_train)\n",
    "print(\"training set standardized\")\n",
    "\n",
    "features_data_val = scaler.transform(features_data_val)\n",
    "print(\"validation set standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros(features_data_train.shape[0])\n",
    "print(y_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 40\n",
    "\n",
    "# Neural network\n",
    "# Define ANN architecture\n",
    "ANN = Sequential()\n",
    "ANN.name = \"ANN\"\n",
    "\n",
    "ANN.add(Dense(1000, activation='relu')) \n",
    "ANN.add(Dropout(0.5))\n",
    "ANN.add(Dense(1000, activation='relu')) \n",
    "ANN.add(Dropout(0.3))\n",
    "ANN.add(Dense(20, activation='relu'))\n",
    "ANN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "ANN.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                                   min_delta=0,\n",
    "                                              patience=3,\n",
    "                                              verbose=0, \n",
    "                                              mode='auto')\n",
    "\n",
    "history = ANN.fit(features_data_train, data_train_label_Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    callbacks= [EarlyStopping],\n",
    "                    validation_data=(features_data_val, data_val_label_Y))\n",
    "\n",
    "ANN.summary() \n",
    "\n",
    "score = ANN.evaluate(features_data_val, data_val_label_Y, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test precision:', score[2])\n",
    "print('Test recall:', score[3])\n",
    "print('Test F1:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for f1\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.plot(history.history['val_f1_m'])\n",
    "plt.title('model F1 score')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "y_predict = ANN.predict(features_data_val)\n",
    "print(y_predict.shape)\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(data_val_label_Y, y_predict)\n",
    "auc = sklearn.metrics.auc(fpr, tpr)\n",
    "roc_auc_score(data_val_label_Y,y_predict)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "AUC = auc\n",
    "    \n",
    "print('AUC = {}'.format(AUC))\n",
    "F1 = f1_score(data_val_label_Y,np.around(y_predict))\n",
    "print('F1 score = {}'.format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_label_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_db = np.loadtxt('features_db_InceptionResNetV2.csv', delimiter = ',', dtype = np.float32)\n",
    "del data_train_index_X\n",
    "del Training_set\n",
    "del data_val_index_X\n",
    "del Validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test element extraction \n",
    "\n",
    "Test_index = np.loadtxt('test_triplets.txt', delimiter = ' ').astype(int)\n",
    "print(Test_index.shape)\n",
    "print(Test_index[:4,:])\n",
    "\n",
    "Test_data = np.zeros([Test_index.shape[0],3,features_db.shape[1]])\n",
    "\n",
    "for i in range(Test_index.shape[0]):\n",
    "    Test_data[i,0,:] = features_db[int(Test_index[i,0]),:]\n",
    "    Test_data[i,1,:] = features_db[int(Test_index[i,1]),:]\n",
    "    Test_data[i,2,:] = features_db[int(Test_index[i,2]),:]\n",
    "    \n",
    "print(Test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data = Test_data.reshape(Test_data.shape[0],-1)\n",
    "print(\"test set reshaped\")\n",
    "Test_data = scaler.transform(Test_data)\n",
    "print(\"test set standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = ANN.predict(Test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predict.txt', np.around(test_predict), delimiter='',fmt = '%d', comments = '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_predict\n",
    "del Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_IML",
   "language": "python",
   "name": "tensorflow_iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
